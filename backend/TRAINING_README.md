# DistilBERT Sentiment Model Training Guide

## ğŸ“‹ Overview

This script trains a **DistilBERT** sentiment analysis model optimized for your **RTX 4050 GPU (6GB VRAM)**. It automatically selects the best dataset, trains efficiently, and saves comprehensive metrics.

## ğŸš€ Quick Start

### 1. Install Required Packages

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets scikit-learn matplotlib seaborn
```

### 2. Run Training

```bash
cd backend
python train_bert.py
```

That's it! The script runs end-to-end automatically.

---

## ğŸ“Š What Gets Created

After training completes, you'll have:

### **`best_model/`** - Your Trained Model
```
best_model/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ vocab.txt
â””â”€â”€ special_tokens_map.json
```

### **`metrics/`** - All Training Metrics
```
metrics/
â”œâ”€â”€ train_results.json          # Training loss and steps
â”œâ”€â”€ eval_results.json           # Accuracy, F1, predictions
â”œâ”€â”€ classification_report.txt   # Precision, recall, F1 per class
â”œâ”€â”€ confusion_matrix.png        # Visual confusion matrix
â”œâ”€â”€ training_loss.png           # Training vs validation loss curves
â”œâ”€â”€ accuracy.png                # Accuracy over time
â”œâ”€â”€ f1_scores.json              # Detailed F1 scores
â”œâ”€â”€ training_time.txt           # Total training time
â””â”€â”€ training_config.json        # All hyperparameters used
```

---

## âš™ï¸ Training Configuration

The script uses these optimized settings:

| Parameter | Value | Why |
|-----------|-------|-----|
| Model | `distilbert-base-uncased` | Fast, efficient, 6x smaller than BERT |
| Epochs | 3 | Optimal for convergence without overfitting |
| Batch Size | 16 | Maximizes RTX 4050 6GB VRAM |
| Learning Rate | 2e-5 | Standard for fine-tuning transformers |
| Max Length | 256 tokens | Balances speed and context |
| FP16 | Enabled | 2x faster training, uses less VRAM |
| Gradient Checkpointing | Enabled | Saves VRAM at slight speed cost |
| Gradient Accumulation | 2 steps | Effective batch size = 32 |

---

## ğŸ“¦ Dataset Selection

The script automatically tries datasets in this order:

1. **IMDB** (50k samples) - **BEST CHOICE**
   - High quality movie reviews
   - Perfect size for 10-20 min training
   - Binary sentiment (Positive/Negative)

2. **Yelp Polarity** (subsampled to 100k)
   - Product/service reviews
   - High quality, diverse

3. **Amazon Polarity** (subsampled to 150k)
   - Large dataset, trimmed for speed
   - Last resort option

**The script picks the first available dataset.**

---

## â±ï¸ Expected Training Time

On **RTX 4050 (6GB VRAM)**:

| Dataset | Samples | Time per Epoch | Total (3 epochs) |
|---------|---------|----------------|------------------|
| IMDB | 25,000 | 7-10 min | **21-30 minutes** |
| Yelp | 100,000 | 25-30 min | 75-90 minutes |
| Amazon | 150,000 | 35-40 min | 105-120 minutes |

---

## ğŸ¯ Expected Performance

After training on IMDB, you should see:

- **Accuracy**: ~90-93%
- **F1 Score**: ~0.90-0.93
- **Confusion Matrix**: Clear diagonal pattern

These are excellent results for sentiment analysis!

---

## ğŸ”§ Using Your Trained Model

### Replace Model in Your Backend

Edit `backend/app.py`:

**Before:**
```python
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

**After:**
```python
model_name = "./best_model"  # Use your trained model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

### Test Your Model

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load your model
tokenizer = AutoTokenizer.from_pretrained("./best_model")
model = AutoModelForSequenceClassification.from_pretrained("./best_model")

# Test it
text = "This product is amazing! I love it!"
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
print(f"Positive: {predictions[0][1].item():.2%}")
print(f"Negative: {predictions[0][0].item():.2%}")
```

---

## ğŸ› Troubleshooting

### "CUDA out of memory"
- Reduce `BATCH_SIZE` from 16 to 8 in `train_bert.py` (line 35)
- Close other GPU applications

### "No module named 'transformers'"
```bash
pip install transformers datasets scikit-learn matplotlib seaborn
```

### "PyTorch not detecting GPU"
```bash
# Reinstall PyTorch with CUDA support
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Training is slow (CPU mode)
- Ensure CUDA drivers are installed
- Check GPU detection in first output section

---

## ğŸ“ˆ Customization

Want to tweak settings? Edit `train_bert.py`:

```python
# Line 33-39: Main configuration
MODEL_NAME = "distilbert-base-uncased"
MAX_LENGTH = 256        # Increase for longer texts
BATCH_SIZE = 16         # Decrease if OOM errors
EPOCHS = 3              # More epochs = better fit (but watch for overfitting)
LEARNING_RATE = 2e-5    # Lower = more stable, higher = faster convergence
```

---

## ğŸ“ Notes

- **First run downloads datasets** (~1-2 GB) - this is normal
- **Model checkpoints saved every epoch** in `./results/`
- **Best model automatically selected** based on accuracy
- **All metrics auto-generated** - no manual work needed

---

## ğŸ’¡ Next Steps

After training:

1. Check `metrics/classification_report.txt` for detailed performance
2. View `metrics/confusion_matrix.png` to see prediction patterns
3. Replace your backend model with the trained one
4. Test on real product reviews to validate performance

---

## ğŸ“ What This Script Does

1. âœ… Detects your GPU and CUDA setup
2. âœ… Automatically selects best available dataset
3. âœ… Estimates training time before starting
4. âœ… Preprocesses and tokenizes data
5. âœ… Trains with optimal settings for RTX 4050
6. âœ… Generates comprehensive metrics and plots
7. âœ… Saves trained model ready for production
8. âœ… Provides clear summary of results

---

## ğŸ“§ Questions?

The script is fully self-contained and verbose - it explains each step as it runs!

**Happy Training! ğŸš€**
